{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing  : First Project\n",
    "# TripAdvisor Recommendation Challenge\n",
    "# Beating BM25\n",
    "#### **Santiago Martin & Léo Ringeissen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "offerings = pd.read_csv('data/offerings.csv',sep=',',header=0)\n",
    "reviews = pd.read_csv('data/reviews.csv',sep=',',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hotel_class</th>\n",
       "      <th>region_id</th>\n",
       "      <th>url</th>\n",
       "      <th>phone</th>\n",
       "      <th>details</th>\n",
       "      <th>address</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>60763</td>\n",
       "      <td>http://www.tripadvisor.com/Hotel_Review-g60763...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'region': 'NY', 'street-address': '147 West 4...</td>\n",
       "      <td>hotel</td>\n",
       "      <td>113317</td>\n",
       "      <td>Casablanca Hotel Times Square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>32655</td>\n",
       "      <td>http://www.tripadvisor.com/Hotel_Review-g32655...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'region': 'CA', 'street-address': '300 S Dohe...</td>\n",
       "      <td>hotel</td>\n",
       "      <td>76049</td>\n",
       "      <td>Four Seasons Hotel Los Angeles at Beverly Hills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.5</td>\n",
       "      <td>60763</td>\n",
       "      <td>http://www.tripadvisor.com/Hotel_Review-g60763...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'region': 'NY', 'street-address': '790 Eighth...</td>\n",
       "      <td>hotel</td>\n",
       "      <td>99352</td>\n",
       "      <td>Hilton Garden Inn Times Square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>60763</td>\n",
       "      <td>http://www.tripadvisor.com/Hotel_Review-g60763...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'region': 'NY', 'street-address': '152 West 5...</td>\n",
       "      <td>hotel</td>\n",
       "      <td>93589</td>\n",
       "      <td>The Michelangelo Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>60763</td>\n",
       "      <td>http://www.tripadvisor.com/Hotel_Review-g60763...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'region': 'NY', 'street-address': '130 West 4...</td>\n",
       "      <td>hotel</td>\n",
       "      <td>217616</td>\n",
       "      <td>The Muse Hotel New York</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hotel_class  region_id                                                url  \\\n",
       "0          4.0      60763  http://www.tripadvisor.com/Hotel_Review-g60763...   \n",
       "1          5.0      32655  http://www.tripadvisor.com/Hotel_Review-g32655...   \n",
       "2          3.5      60763  http://www.tripadvisor.com/Hotel_Review-g60763...   \n",
       "3          4.0      60763  http://www.tripadvisor.com/Hotel_Review-g60763...   \n",
       "4          4.0      60763  http://www.tripadvisor.com/Hotel_Review-g60763...   \n",
       "\n",
       "   phone  details                                            address   type  \\\n",
       "0    NaN      NaN  {'region': 'NY', 'street-address': '147 West 4...  hotel   \n",
       "1    NaN      NaN  {'region': 'CA', 'street-address': '300 S Dohe...  hotel   \n",
       "2    NaN      NaN  {'region': 'NY', 'street-address': '790 Eighth...  hotel   \n",
       "3    NaN      NaN  {'region': 'NY', 'street-address': '152 West 5...  hotel   \n",
       "4    NaN      NaN  {'region': 'NY', 'street-address': '130 West 4...  hotel   \n",
       "\n",
       "       id                                             name  \n",
       "0  113317                    Casablanca Hotel Times Square  \n",
       "1   76049  Four Seasons Hotel Los Angeles at Beverly Hills  \n",
       "2   99352                   Hilton Garden Inn Times Square  \n",
       "3   93589                           The Michelangelo Hotel  \n",
       "4  217616                          The Muse Hotel New York  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offerings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>date_stayed</th>\n",
       "      <th>offering_id</th>\n",
       "      <th>num_helpful_votes</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>via_mobile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'service': 5.0, 'cleanliness': 5.0, 'overall'...</td>\n",
       "      <td>“Truly is \"Jewel of the Upper Wets Side\"”</td>\n",
       "      <td>Stayed in a king suite for 11 nights and yes i...</td>\n",
       "      <td>{'username': 'Papa_Panda', 'num_cities': 22, '...</td>\n",
       "      <td>December 2012</td>\n",
       "      <td>93338</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-12-17</td>\n",
       "      <td>147643103</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'service': 5.0, 'cleanliness': 5.0, 'overall'...</td>\n",
       "      <td>“My home away from home!”</td>\n",
       "      <td>On every visit to NYC, the Hotel Beacon is the...</td>\n",
       "      <td>{'username': 'Maureen V', 'num_reviews': 2, 'n...</td>\n",
       "      <td>December 2012</td>\n",
       "      <td>93338</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-12-17</td>\n",
       "      <td>147639004</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'service': 4.0, 'cleanliness': 5.0, 'overall'...</td>\n",
       "      <td>“Great Stay”</td>\n",
       "      <td>This is a great property in Midtown. We two di...</td>\n",
       "      <td>{'username': 'vuguru', 'num_cities': 12, 'num_...</td>\n",
       "      <td>December 2012</td>\n",
       "      <td>1762573</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-12-18</td>\n",
       "      <td>147697954</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'service': 5.0, 'cleanliness': 5.0, 'overall'...</td>\n",
       "      <td>“Modern Convenience”</td>\n",
       "      <td>The Andaz is a nice hotel in a central locatio...</td>\n",
       "      <td>{'username': 'Hotel-Designer', 'num_cities': 5...</td>\n",
       "      <td>August 2012</td>\n",
       "      <td>1762573</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-12-17</td>\n",
       "      <td>147625723</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'service': 4.0, 'cleanliness': 5.0, 'overall'...</td>\n",
       "      <td>“Its the best of the Andaz Brand in the US....”</td>\n",
       "      <td>I have stayed at each of the US Andaz properti...</td>\n",
       "      <td>{'username': 'JamesE339', 'num_cities': 34, 'n...</td>\n",
       "      <td>December 2012</td>\n",
       "      <td>1762573</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-12-17</td>\n",
       "      <td>147612823</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ratings  \\\n",
       "0  {'service': 5.0, 'cleanliness': 5.0, 'overall'...   \n",
       "1  {'service': 5.0, 'cleanliness': 5.0, 'overall'...   \n",
       "2  {'service': 4.0, 'cleanliness': 5.0, 'overall'...   \n",
       "3  {'service': 5.0, 'cleanliness': 5.0, 'overall'...   \n",
       "4  {'service': 4.0, 'cleanliness': 5.0, 'overall'...   \n",
       "\n",
       "                                             title  \\\n",
       "0        “Truly is \"Jewel of the Upper Wets Side\"”   \n",
       "1                        “My home away from home!”   \n",
       "2                                     “Great Stay”   \n",
       "3                             “Modern Convenience”   \n",
       "4  “Its the best of the Andaz Brand in the US....”   \n",
       "\n",
       "                                                text  \\\n",
       "0  Stayed in a king suite for 11 nights and yes i...   \n",
       "1  On every visit to NYC, the Hotel Beacon is the...   \n",
       "2  This is a great property in Midtown. We two di...   \n",
       "3  The Andaz is a nice hotel in a central locatio...   \n",
       "4  I have stayed at each of the US Andaz properti...   \n",
       "\n",
       "                                              author    date_stayed  \\\n",
       "0  {'username': 'Papa_Panda', 'num_cities': 22, '...  December 2012   \n",
       "1  {'username': 'Maureen V', 'num_reviews': 2, 'n...  December 2012   \n",
       "2  {'username': 'vuguru', 'num_cities': 12, 'num_...  December 2012   \n",
       "3  {'username': 'Hotel-Designer', 'num_cities': 5...    August 2012   \n",
       "4  {'username': 'JamesE339', 'num_cities': 34, 'n...  December 2012   \n",
       "\n",
       "   offering_id  num_helpful_votes        date         id  via_mobile  \n",
       "0        93338                  0  2012-12-17  147643103       False  \n",
       "1        93338                  0  2012-12-17  147639004       False  \n",
       "2      1762573                  0  2012-12-18  147697954       False  \n",
       "3      1762573                  0  2012-12-17  147625723       False  \n",
       "4      1762573                  0  2012-12-17  147612823       False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_offerings = ['hotel_class', 'region_id', 'url', 'phone', 'details', 'address', 'type']\n",
    "offerings = offerings.drop(cols_to_drop_offerings, axis=1)\n",
    "\n",
    "cols_to_drop_review = ['author', 'date_stayed', 'num_helpful_votes', 'date', 'id', 'via_mobile']\n",
    "reviews = reviews.drop(cols_to_drop_review, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create rating columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "required_keys = ['service', 'cleanliness', 'overall', 'value', 'location', 'sleep_quality', 'rooms']\n",
    "\n",
    "# Function to preprocess the ratings column\n",
    "def preprocess_ratings(dataframe):\n",
    "    # Convert the 'ratings' column from string to dictionary using ast.literal_eval\n",
    "    dataframe['ratings_dict'] = dataframe['ratings'].apply(lambda x: ast.literal_eval(x))\n",
    "    \n",
    "    # Define the required keys\n",
    "    \n",
    "    # Extract only the required keys from the dictionary\n",
    "    dataframe['filtered_ratings_dict'] = dataframe['ratings_dict'].apply(\n",
    "        lambda d: {key: d[key] for key in required_keys if key in d}\n",
    "    )\n",
    "    \n",
    "    # Normalize the filtered dictionary column into separate columns\n",
    "    ratings_expanded = pd.json_normalize(dataframe['filtered_ratings_dict'])\n",
    "    \n",
    "    # Drop rows where any of the required keys are missing\n",
    "    ratings_filtered = ratings_expanded.dropna(subset=required_keys)\n",
    "    \n",
    "    # Add the expanded columns back to the original dataframe\n",
    "    dataframe = dataframe.join(ratings_filtered)\n",
    "    \n",
    "    # Drop the intermediate columns\n",
    "    dataframe = dataframe.drop(columns=['ratings', 'ratings_dict', 'filtered_ratings_dict'])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# Apply the preprocessing function\n",
    "reviews = preprocess_ratings(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title                 0\n",
      "text                  0\n",
      "offering_id           0\n",
      "service          442170\n",
      "cleanliness      442170\n",
      "overall          442170\n",
      "value            442170\n",
      "location         442170\n",
      "sleep_quality    442170\n",
      "rooms            442170\n",
      "dtype: int64\n",
      "title            0\n",
      "text             0\n",
      "offering_id      0\n",
      "service          0\n",
      "cleanliness      0\n",
      "overall          0\n",
      "value            0\n",
      "location         0\n",
      "sleep_quality    0\n",
      "rooms            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(reviews.isnull().sum())\n",
    "reviews.dropna(inplace=True)\n",
    "print(reviews.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining offerings and reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>service</th>\n",
       "      <th>cleanliness</th>\n",
       "      <th>overall</th>\n",
       "      <th>value</th>\n",
       "      <th>location</th>\n",
       "      <th>sleep_quality</th>\n",
       "      <th>rooms</th>\n",
       "      <th>text</th>\n",
       "      <th>num_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72572</td>\n",
       "      <td>BEST WESTERN PLUS Pioneer Square Hotel</td>\n",
       "      <td>4.601010</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>4.388889</td>\n",
       "      <td>4.323232</td>\n",
       "      <td>4.570707</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.282828</td>\n",
       "      <td>I had to make fast visit to seattle and I foun...</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72579</td>\n",
       "      <td>BEST WESTERN Loyal Inn</td>\n",
       "      <td>4.232000</td>\n",
       "      <td>4.240000</td>\n",
       "      <td>3.888000</td>\n",
       "      <td>4.152000</td>\n",
       "      <td>4.192000</td>\n",
       "      <td>3.768000</td>\n",
       "      <td>3.856000</td>\n",
       "      <td>Great service, rooms were clean, could use som...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72586</td>\n",
       "      <td>BEST WESTERN PLUS Executive Inn</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.287879</td>\n",
       "      <td>4.045455</td>\n",
       "      <td>4.053030</td>\n",
       "      <td>4.537879</td>\n",
       "      <td>4.113636</td>\n",
       "      <td>3.992424</td>\n",
       "      <td>Beautiful views of the space needle - especial...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72598</td>\n",
       "      <td>Comfort Inn &amp; Suites Seattle</td>\n",
       "      <td>3.243243</td>\n",
       "      <td>3.243243</td>\n",
       "      <td>2.918919</td>\n",
       "      <td>3.054054</td>\n",
       "      <td>3.027027</td>\n",
       "      <td>3.270270</td>\n",
       "      <td>3.189189</td>\n",
       "      <td>This hotel is in need of some serious updates....</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73236</td>\n",
       "      <td>Days Inn San Antonio/Near Lackland AFB</td>\n",
       "      <td>4.277778</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>3.777778</td>\n",
       "      <td>4.111111</td>\n",
       "      <td>3.722222</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>My experience at this days inn was perfect. th...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                    name   service  cleanliness  \\\n",
       "0  72572  BEST WESTERN PLUS Pioneer Square Hotel  4.601010     4.636364   \n",
       "1  72579                  BEST WESTERN Loyal Inn  4.232000     4.240000   \n",
       "2  72586         BEST WESTERN PLUS Executive Inn  4.250000     4.287879   \n",
       "3  72598            Comfort Inn & Suites Seattle  3.243243     3.243243   \n",
       "4  73236  Days Inn San Antonio/Near Lackland AFB  4.277778     3.111111   \n",
       "\n",
       "    overall     value  location  sleep_quality     rooms  \\\n",
       "0  4.388889  4.323232  4.570707       4.333333  4.282828   \n",
       "1  3.888000  4.152000  4.192000       3.768000  3.856000   \n",
       "2  4.045455  4.053030  4.537879       4.113636  3.992424   \n",
       "3  2.918919  3.054054  3.027027       3.270270  3.189189   \n",
       "4  3.388889  3.777778  4.111111       3.722222  3.222222   \n",
       "\n",
       "                                                text  num_reviews  \n",
       "0  I had to make fast visit to seattle and I foun...          198  \n",
       "1  Great service, rooms were clean, could use som...          125  \n",
       "2  Beautiful views of the space needle - especial...          132  \n",
       "3  This hotel is in need of some serious updates....           37  \n",
       "4  My experience at this days inn was perfect. th...           18  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the reviews and offerings dataframes on the offering_id and id columns\n",
    "merged_df = pd.merge(reviews, offerings, left_on='offering_id', right_on='id')\n",
    "\n",
    "# Group by the hotel id and name, and calculate the mean for the rating columns and count for the number of reviews\n",
    "grouped_df = merged_df.groupby(['id', 'name']).agg(\n",
    "    {cat: 'mean' for cat in required_keys} | {'text': ' '.join} | {'offering_id': 'count'}\n",
    ").rename(columns={'offering_id': 'num_reviews'}).reset_index()\n",
    "\n",
    "# Display the resulting dataframe\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3754, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>service</th>\n",
       "      <th>cleanliness</th>\n",
       "      <th>overall</th>\n",
       "      <th>value</th>\n",
       "      <th>location</th>\n",
       "      <th>sleep_quality</th>\n",
       "      <th>rooms</th>\n",
       "      <th>text</th>\n",
       "      <th>num_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72572</td>\n",
       "      <td>BEST WESTERN PLUS Pioneer Square Hotel</td>\n",
       "      <td>4.601010</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>4.388889</td>\n",
       "      <td>4.323232</td>\n",
       "      <td>4.570707</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.282828</td>\n",
       "      <td>I had to make fast visit to seattle and I foun...</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72579</td>\n",
       "      <td>BEST WESTERN Loyal Inn</td>\n",
       "      <td>4.232000</td>\n",
       "      <td>4.240000</td>\n",
       "      <td>3.888000</td>\n",
       "      <td>4.152000</td>\n",
       "      <td>4.192000</td>\n",
       "      <td>3.768000</td>\n",
       "      <td>3.856000</td>\n",
       "      <td>Great service, rooms were clean, could use som...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72586</td>\n",
       "      <td>BEST WESTERN PLUS Executive Inn</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.287879</td>\n",
       "      <td>4.045455</td>\n",
       "      <td>4.053030</td>\n",
       "      <td>4.537879</td>\n",
       "      <td>4.113636</td>\n",
       "      <td>3.992424</td>\n",
       "      <td>Beautiful views of the space needle - especial...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72598</td>\n",
       "      <td>Comfort Inn &amp; Suites Seattle</td>\n",
       "      <td>3.243243</td>\n",
       "      <td>3.243243</td>\n",
       "      <td>2.918919</td>\n",
       "      <td>3.054054</td>\n",
       "      <td>3.027027</td>\n",
       "      <td>3.270270</td>\n",
       "      <td>3.189189</td>\n",
       "      <td>This hotel is in need of some serious updates....</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73236</td>\n",
       "      <td>Days Inn San Antonio/Near Lackland AFB</td>\n",
       "      <td>4.277778</td>\n",
       "      <td>3.111111</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>3.777778</td>\n",
       "      <td>4.111111</td>\n",
       "      <td>3.722222</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>My experience at this days inn was perfect. th...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                    name   service  cleanliness  \\\n",
       "0  72572  BEST WESTERN PLUS Pioneer Square Hotel  4.601010     4.636364   \n",
       "1  72579                  BEST WESTERN Loyal Inn  4.232000     4.240000   \n",
       "2  72586         BEST WESTERN PLUS Executive Inn  4.250000     4.287879   \n",
       "3  72598            Comfort Inn & Suites Seattle  3.243243     3.243243   \n",
       "4  73236  Days Inn San Antonio/Near Lackland AFB  4.277778     3.111111   \n",
       "\n",
       "    overall     value  location  sleep_quality     rooms  \\\n",
       "0  4.388889  4.323232  4.570707       4.333333  4.282828   \n",
       "1  3.888000  4.152000  4.192000       3.768000  3.856000   \n",
       "2  4.045455  4.053030  4.537879       4.113636  3.992424   \n",
       "3  2.918919  3.054054  3.027027       3.270270  3.189189   \n",
       "4  3.388889  3.777778  4.111111       3.722222  3.222222   \n",
       "\n",
       "                                                text  num_reviews  \n",
       "0  I had to make fast visit to seattle and I foun...          198  \n",
       "1  Great service, rooms were clean, could use som...          125  \n",
       "2  Beautiful views of the space needle - especial...          132  \n",
       "3  This hotel is in need of some serious updates....           37  \n",
       "4  My experience at this days inn was perfect. th...           18  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(grouped_df.shape)\n",
    "display(grouped_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing 100 random queries and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = grouped_df['text'].tolist()\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying on N documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2619, 456, 102, 3037, 1126, 1003, 914, 571, 3016, 419, 2771, 3033, 3654, 2233, 356, 2418, 1728, 130, 122, 383, 895, 952, 2069, 2465, 108, 2298, 814, 2932, 2661, 2872, 2232, 1718, 902, 1839, 2413, 1139, 3315, 3560, 26, 3108, 3300, 653, 2859, 1731, 1393, 1138, 636, 881, 3127, 1378, 418, 379, 1556, 396, 1470, 3471, 1408, 2472, 1083, 3305, 177, 2988, 1881, 2196, 511, 1550, 322, 2261, 1200, 3397, 2574, 2533, 3626, 3529, 1481, 2364, 787, 2885, 284, 187, 2708, 933, 3166, 1185, 326, 3503, 953, 3549, 413, 1857, 2603, 3416, 1494, 666, 1516, 1455, 858, 2745, 1093, 2874]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n_queries = 100 # For testing, we will use 10 queries\n",
    "random.seed(42)\n",
    "query_ids = random.sample(range(len(corpus)), n_queries)\n",
    "print(query_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    regex_tokenizer = RegexpTokenizer('\\w\\w+')\n",
    "    text = regex_tokenizer.tokenize(text)\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    stemmer = PorterStemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = [clean_text(doc) for doc in corpus]\n",
    "processed_tokenized_corpus = [doc.split(\" \") for doc in processed_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25\n",
    "\n",
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5390851523430844\n",
      "[2727, 867, 546, 718, 598, 232, 2657, 1833, 244, 163, 832, 814, 3020, 2239, 371, 2886, 239, 149, 551, 316, 1619, 156, 1310, 3296, 1761, 601, 1156, 1622, 3192, 3081, 232, 3648, 531, 2023, 316, 1144, 2023, 137, 1010, 2023, 3188, 718, 436, 341, 1622, 2639, 607, 535, 718, 2850, 489, 208, 1622, 371, 404, 1717, 3081, 464, 3140, 1222, 460, 1691, 239, 607, 509, 426, 464, 1619, 239, 157, 239, 1334, 2639, 1761, 2023, 382, 1156, 2032, 323, 3656, 1606, 607, 179, 1222, 221, 1622, 1606, 3579, 555, 489, 3020, 1640, 601, 1334, 1010, 1192, 841, 2431, 323, 804]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "mses = []\n",
    "best_doc_ids = []\n",
    "\n",
    "for query_id in query_ids:\n",
    "    query = corpus[query_id]\n",
    "    tokenized_query = query.split(\" \")\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    doc_id = doc_scores.argsort()[-2]\n",
    "    best_doc_ids.append(doc_id)\n",
    "    mse = np.mean((grouped_df.iloc[query_id][required_keys] - grouped_df.iloc[doc_id][required_keys])**2)\n",
    "    mses.append(mse)\n",
    "    \n",
    "# Calculate the mean mse\n",
    "mean_mse_bm25_baseline = np.mean(mses)\n",
    "print(mean_mse_bm25_baseline)\n",
    "print(best_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Score: 0.5390851523430844\n",
    "- Best IDs: [2727, 867, 546, 718, 598, 232, 2657, 1833, 244, 163, 832, 814, 3020, 2239, 371, 2886, 239, 149, 551, 316, 1619, 156, 1310, 3296, 1761, 601, 1156, 1622, 3192, 3081, 232, 3648, 531, 2023, 316, 1144, 2023, 137, 1010, 2023, 3188, 718, 436, 341, 1622, 2639, 607, 535, 718, 2850, 489, 208, 1622, 371, 404, 1717, 3081, 464, 3140, 1222, 460, 1691, 239, 607, 509, 426, 464, 1619, 239, 157, 239, 1334, 2639, 1761, 2023, 382, 1156, 2032, 323, 3656, 1606, 607, 179, 1222, 221, 1622, 1606, 3579, 555, 489, 3020, 1640, 601, 1334, 1010, 1192, 841, 2431, 323, 804]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.418855258103762\n",
      "[2727, 960, 546, 3603, 615, 239, 1874, 1833, 244, 2023, 832, 1276, 3020, 714, 371, 2886, 239, 149, 551, 316, 2618, 383, 1310, 3296, 1761, 1622, 1156, 1761, 1683, 3081, 1010, 3648, 1761, 573, 115, 1144, 1622, 115, 1010, 539, 3188, 718, 3258, 341, 1622, 2639, 3603, 1606, 3020, 3656, 3442, 208, 1622, 371, 867, 2105, 601, 1310, 1719, 1222, 460, 1691, 1691, 895, 506, 573, 464, 239, 2882, 157, 867, 1334, 1622, 1761, 817, 260, 824, 685, 323, 3656, 1606, 607, 2239, 1222, 221, 1622, 867, 1266, 555, 489, 3020, 3188, 1622, 1334, 1010, 3156, 841, 2431, 1619, 804]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "processed_bm25 = BM25Okapi(processed_tokenized_corpus)\n",
    "\n",
    "processed_mses = []\n",
    "processed_best_doc_ids = []\n",
    "\n",
    "for query_id in query_ids:\n",
    "    query = processed_corpus[query_id]\n",
    "    tokenized_query = query.split(\" \")\n",
    "    doc_scores = processed_bm25.get_scores(tokenized_query)\n",
    "    doc_id = doc_scores.argsort()[-2]\n",
    "    processed_best_doc_ids.append(doc_id)\n",
    "    mse = np.mean((grouped_df.iloc[query_id][required_keys] - grouped_df.iloc[doc_id][required_keys])**2)\n",
    "    processed_mses.append(mse)\n",
    "    \n",
    "# Calculate the mean mse\n",
    "mean_mse_bm25_preprocessed = np.mean(processed_mses)\n",
    "print(mean_mse_bm25_preprocessed)\n",
    "print(processed_best_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Score: 0.418855258103762\n",
    "\n",
    "- Best IDs: [2727, 960, 546, 3603, 615, 239, 1874, 1833, 244, 2023, 832, 1276, 3020, 714, 371, 2886, 239, 149, 551, 316, 2618, 383, 1310, 3296, 1761, 1622, 1156, 1761, 1683, 3081, 1010, 3648, 1761, 573, 115, 1144, 1622, 115, 1010, 539, 3188, 718, 3258, 341, 1622, 2639, 3603, 1606, 3020, 3656, 3442, 208, 1622, 371, 867, 2105, 601, 1310, 1719, 1222, 460, 1691, 1691, 895, 506, 573, 464, 239, 2882, 157, 867, 1334, 1622, 1761, 817, 260, 824, 685, 323, 3656, 1606, 607, 2239, 1222, 221, 1622, 867, 1266, 555, 489, 3020, 3188, 1622, 1334, 1010, 3156, 841, 2431, 1619, 804]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing TF-IDF\n",
    "\n",
    "### No Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4926256741935092\n",
      "[1602, 704, 2365, 767, 598, 2735, 1874, 1833, 2610, 179, 2838, 3529, 3351, 2827, 371, 178, 110, 1640, 3211, 1774, 1277, 253, 95, 3296, 405, 2580, 3484, 3077, 3247, 3081, 2727, 3648, 1646, 1831, 3356, 1144, 3231, 2662, 2889, 179, 3188, 756, 591, 364, 962, 1140, 640, 1277, 179, 2850, 962, 378, 2139, 371, 3607, 2105, 179, 464, 1536, 2735, 1790, 1996, 1640, 892, 512, 464, 95, 3124, 2226, 1749, 464, 1053, 1984, 1413, 2370, 285, 2437, 3391, 298, 100, 2258, 636, 3433, 1189, 221, 2570, 3607, 1273, 128, 512, 3351, 3188, 3209, 1640, 3190, 3572, 100, 1606, 1028, 2032]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Build TF-IDF matrix\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "\n",
    "mses = []\n",
    "best_doc_ids = []\n",
    "\n",
    "# Iterate through random queries\n",
    "for query_id in query_ids:\n",
    "    query = corpus[query_id]\n",
    "    query_vector = tfidf.transform([query])\n",
    "    doc_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    doc_id = np.argsort(doc_scores)[-2]  # Second-highest score\n",
    "    best_doc_ids.append(doc_id)\n",
    "    mse = np.mean((grouped_df.iloc[query_id][required_keys] - grouped_df.iloc[doc_id][required_keys])**2)\n",
    "    mses.append(mse)\n",
    "\n",
    "mean_mse_tf_baseline = np.mean(mses)\n",
    "print(mean_mse_tf_baseline)\n",
    "print(best_doc_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Score: 0.4926256741935092\n",
    "\n",
    "- Best IDs: [1602, 704, 2365, 767, 598, 2735, 1874, 1833, 2610, 179, 2838, 3529, 3351, 2827, 371, 178, 110, 1640, 3211, 1774, 1277, 253, 95, 3296, 405, 2580, 3484, 3077, 3247, 3081, 2727, 3648, 1646, 1831, 3356, 1144, 3231, 2662, 2889, 179, 3188, 756, 591, 364, 962, 1140, 640, 1277, 179, 2850, 962, 378, 2139, 371, 3607, 2105, 179, 464, 1536, 2735, 1790, 1996, 1640, 892, 512, 464, 95, 3124, 2226, 1749, 464, 1053, 1984, 1413, 2370, 285, 2437, 3391, 298, 100, 2258, 636, 3433, 1189, 221, 2570, 3607, 1273, 128, 512, 3351, 3188, 3209, 1640, 3190, 3572, 100, 1606, 1028, 2032]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4355368491613478\n",
      "[1602, 937, 1094, 767, 2906, 1006, 1874, 1833, 2610, 178, 2838, 3529, 3351, 2571, 371, 221, 110, 817, 3211, 1774, 892, 253, 2107, 3296, 1597, 2580, 3484, 3077, 3247, 3081, 2727, 3648, 1646, 1831, 825, 1144, 3145, 3108, 2107, 1795, 3188, 756, 290, 3125, 1087, 2730, 1041, 1277, 3336, 2850, 2602, 378, 2283, 371, 3607, 2279, 2426, 943, 1536, 2244, 1790, 1749, 1702, 892, 512, 95, 2643, 3124, 2244, 1884, 95, 1882, 1984, 1413, 3389, 2428, 2437, 1225, 1982, 100, 19, 636, 3433, 1189, 248, 2570, 2643, 2362, 128, 512, 3351, 3188, 2577, 3145, 3190, 3572, 833, 2431, 1028, 2032]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Build TF-IDF matrix\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(processed_corpus)\n",
    "\n",
    "mses = []\n",
    "best_doc_ids = []\n",
    "\n",
    "# Iterate through random queries\n",
    "for query_id in query_ids:\n",
    "    query = processed_corpus[query_id]\n",
    "    query_vector = tfidf.transform([query])\n",
    "    doc_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    doc_id = np.argsort(doc_scores)[-2]  # Second-highest score\n",
    "    best_doc_ids.append(doc_id)\n",
    "    mse = np.mean((grouped_df.iloc[query_id][required_keys] - grouped_df.iloc[doc_id][required_keys])**2)\n",
    "    mses.append(mse)\n",
    "\n",
    "mean_mse_tf_preprocessed = np.mean(mses)\n",
    "print(mean_mse_tf_preprocessed)\n",
    "print(best_doc_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Score: 0.4355368491613478\n",
    "\n",
    "- Best IDs: [1602, 937, 1094, 767, 2906, 1006, 1874, 1833, 2610, 178, 2838, 3529, 3351, 2571, 371, 221, 110, 817, 3211, 1774, 892, 253, 2107, 3296, 1597, 2580, 3484, 3077, 3247, 3081, 2727, 3648, 1646, 1831, 825, 1144, 3145, 3108, 2107, 1795, 3188, 756, 290, 3125, 1087, 2730, 1041, 1277, 3336, 2850, 2602, 378, 2283, 371, 3607, 2279, 2426, 943, 1536, 2244, 1790, 1749, 1702, 892, 512, 95, 2643, 3124, 2244, 1884, 95, 1882, 1984, 1413, 3389, 2428, 2437, 1225, 1982, 100, 19, 636, 3433, 1189, 248, 2570, 2643, 2362, 128, 512, 3351, 3188, 2577, 3145, 3190, 3572, 833, 2431, 1028, 2032]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flan T5\n",
    "\n",
    "## Small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ringi_3xz04z7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import sentencepiece\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f0954b24234a05848b4db9a92abcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ringi_3xz04z7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ringi_3xz04z7\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea308e799b6442ea825e56dcb5b9deff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcf305dbfc64503aaa033d8bd9dc4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a12ef4bbef54c8593c5c4afa2ad73de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5940aeb6894c45c985e1b757dc591c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents into embeddings...\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import sentencepiece\n",
    "import torch\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "model_name = \"t5-small\"  # Change to \"t5-base\" or \"t5-large\" for better results\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5Model.from_pretrained(model_name)\n",
    "\n",
    "# Ensure reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function to encode text into embeddings using T5\n",
    "def encode_text(text, max_length=512):\n",
    "    # Tokenize and prepare inputs\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    # Pass inputs through T5 encoder to get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encoder(**inputs)\n",
    "    # Mean pooling of the token embeddings\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Preprocess corpus to get embeddings\n",
    "print(\"Encoding documents into embeddings...\")\n",
    "document_embeddings = np.array([encode_text(doc) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing queries...\n",
      "Mean MSE: 0.6340350221023886\n",
      "Best doc IDs: [2333, 560, 1016, 945, 1796, 594, 657, 1619, 163, 1488, 3702, 670, 3454, 1122, 310, 141, 2303, 2229, 540, 1724, 1713, 1369, 48, 2208, 1182, 3462, 114, 1599, 2541, 2344, 1625, 2348, 1919, 3702, 62, 2074, 3643, 2530, 504, 364, 623, 856, 1978, 2788, 1126, 287, 3421, 197, 713, 38, 2038, 2829, 2125, 69, 951, 170, 832, 821, 3437, 2998, 2663, 3521, 461, 3659, 1904, 864, 3208, 2510, 2880, 754, 442, 2605, 3463, 499, 1544, 1161, 830, 3748, 695, 699, 1791, 2681, 2434, 1974, 1475, 2003, 1070, 978, 2679, 1796, 2702, 3404, 998, 1872, 2151, 1494, 2581, 37, 2062, 1550]\n"
     ]
    }
   ],
   "source": [
    "mses = []\n",
    "best_doc_ids = []\n",
    "\n",
    "# Iterate through random queries\n",
    "print(\"Processing queries...\")\n",
    "for query_id in query_ids:\n",
    "    query = corpus[query_id]\n",
    "    query_embedding = encode_text(query)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    doc_scores = cosine_similarity([query_embedding], document_embeddings).flatten()\n",
    "    \n",
    "    # Get the second-highest score\n",
    "    doc_id = np.argsort(doc_scores)[-2]  # Second-highest score\n",
    "    best_doc_ids.append(doc_id)\n",
    "    \n",
    "    # Calculate MSE for evaluation\n",
    "    mse = np.mean((grouped_df.iloc[query_id][required_keys] - grouped_df.iloc[doc_id][required_keys])**2)\n",
    "    mses.append(mse)\n",
    "\n",
    "# Calculate the mean MSE\n",
    "mean_mse_t5s_baseline = np.mean(mses)\n",
    "print(f\"Mean MSE: {mean_mse_t5s_baseline}\")\n",
    "print(\"Best doc IDs:\", best_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Score: 0.6340350221023886\n",
    "\n",
    "- Best IDs: [2333, 560, 1016, 945, 1796, 594, 657, 1619, 163, 1488, 3702, 670, 3454, 1122, 310, 141, 2303, 2229, 540, 1724, 1713, 1369, 48, 2208, 1182, 3462, 114, 1599, 2541, 2344, 1625, 2348, 1919, 3702, 62, 2074, 3643, 2530, 504, 364, 623, 856, 1978, 2788, 1126, 287, 3421, 197, 713, 38, 2038, 2829, 2125, 69, 951, 170, 832, 821, 3437, 2998, 2663, 3521, 461, 3659, 1904, 864, 3208, 2510, 2880, 754, 442, 2605, 3463, 499, 1544, 1161, 830, 3748, 695, 699, 1791, 2681, 2434, 1974, 1475, 2003, 1070, 978, 2679, 1796, 2702, 3404, 998, 1872, 2151, 1494, 2581, 37, 2062, 1550]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents into embeddings...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load T5 model and tokenizer\n",
    "model_name = \"t5-small\"  # Change to \"t5-base\" or \"t5-large\" for better results\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5Model.from_pretrained(model_name)\n",
    "\n",
    "# Ensure reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function to encode text into embeddings using T5\n",
    "def encode_text(text, max_length=512):\n",
    "    # Tokenize and prepare inputs\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    # Pass inputs through T5 encoder to get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encoder(**inputs)\n",
    "    # Mean pooling of the token embeddings\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Preprocess corpus to get embeddings\n",
    "print(\"Encoding documents into embeddings...\")\n",
    "document_embeddings = np.array([encode_text(doc) for doc in processed_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing queries...\n",
      "Mean MSE: 0.5792905873745835\n",
      "Best doc IDs: [2242, 1123, 406, 2810, 2955, 2438, 1446, 2343, 167, 45, 1155, 2084, 3564, 663, 639, 3169, 1604, 1148, 551, 959, 1606, 1369, 943, 2304, 1239, 1262, 1228, 3281, 2184, 3453, 902, 3139, 2232, 3596, 3526, 959, 2665, 762, 2122, 962, 1027, 1846, 1831, 607, 3314, 1262, 110, 1288, 3092, 3052, 2460, 3226, 2086, 2010, 766, 2783, 1264, 2928, 596, 159, 2983, 2082, 1559, 3029, 800, 936, 430, 820, 1004, 3438, 853, 1820, 3682, 458, 1439, 3450, 830, 3549, 493, 1156, 904, 2086, 2810, 2783, 1850, 3564, 2204, 3326, 1997, 2783, 2153, 1735, 1496, 2508, 506, 581, 935, 698, 2092, 2435]\n"
     ]
    }
   ],
   "source": [
    "mses = []\n",
    "best_doc_ids = []\n",
    "\n",
    "# Iterate through random queries\n",
    "print(\"Processing queries...\")\n",
    "for query_id in query_ids:\n",
    "    query = processed_corpus[query_id]\n",
    "    query_embedding = encode_text(query)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    doc_scores = cosine_similarity([query_embedding], document_embeddings).flatten()\n",
    "    \n",
    "    # Get the second-highest score\n",
    "    doc_id = np.argsort(doc_scores)[-2]  # Second-highest score\n",
    "    best_doc_ids.append(doc_id)\n",
    "    \n",
    "    # Calculate MSE for evaluation\n",
    "    mse = np.mean((grouped_df.iloc[query_id][required_keys] - grouped_df.iloc[doc_id][required_keys])**2)\n",
    "    mses.append(mse)\n",
    "\n",
    "# Calculate the mean MSE\n",
    "mean_mse_t5s_preprocessed = np.mean(mses)\n",
    "print(f\"Mean MSE: {mean_mse_t5s_preprocessed}\")\n",
    "print(\"Best doc IDs:\", best_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Score: 0.5792905873745835\n",
    "\n",
    "- Best IDs: [2242, 1123, 406, 2810, 2955, 2438, 1446, 2343, 167, 45, 1155, 2084, 3564, 663, 639, 3169, 1604, 1148, 551, 959, 1606, 1369, 943, 2304, 1239, 1262, 1228, 3281, 2184, 3453, 902, 3139, 2232, 3596, 3526, 959, 2665, 762, 2122, 962, 1027, 1846, 1831, 607, 3314, 1262, 110, 1288, 3092, 3052, 2460, 3226, 2086, 2010, 766, 2783, 1264, 2928, 596, 159, 2983, 2082, 1559, 3029, 800, 936, 430, 820, 1004, 3438, 853, 1820, 3682, 458, 1439, 3450, 830, 3549, 493, 1156, 904, 2086, 2810, 2783, 1850, 3564, 2204, 3326, 1997, 2783, 2153, 1735, 1496, 2508, 506, 581, 935, 698, 2092, 2435]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d8199f032b401192146ba0870884f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ringi_3xz04z7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ringi_3xz04z7\\.cache\\huggingface\\hub\\models--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a557bce56c394bf3a21ecd0822f2e89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9945c2866742f5a6767ca3d9fe1765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da03491357a749d499012f71e6cd7ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents into embeddings...\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import sentencepiece\n",
    "import torch\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "model_name = \"t5-base\"  # Change to \"t5-base\" or \"t5-large\" for better results\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5Model.from_pretrained(model_name)\n",
    "\n",
    "# Ensure reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function to encode text into embeddings using T5\n",
    "def encode_text(text, max_length=512):\n",
    "    # Tokenize and prepare inputs\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    # Pass inputs through T5 encoder to get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encoder(**inputs)\n",
    "    # Mean pooling of the token embeddings\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Preprocess corpus to get embeddings\n",
    "print(\"Encoding documents into embeddings...\")\n",
    "document_embeddings = np.array([encode_text(doc) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing queries...\n",
      "Mean MSE: 0.5515936991289846\n",
      "Best doc IDs: [2639, 1433, 1945, 504, 2100, 1801, 2631, 3638, 1732, 3606, 3084, 1491, 639, 1887, 200, 3080, 1028, 2957, 1711, 354, 3584, 2978, 3313, 2750, 1182, 644, 114, 2772, 2541, 470, 1417, 2348, 305, 1053, 430, 2307, 2655, 2530, 628, 2928, 0, 2141, 83, 249, 181, 436, 594, 557, 754, 126, 1303, 530, 1404, 2962, 1523, 1971, 372, 3191, 2969, 3503, 1438, 3521, 645, 594, 165, 1317, 3208, 622, 3502, 230, 812, 3100, 3135, 1352, 3282, 3387, 3362, 3595, 903, 3488, 2064, 639, 1935, 1319, 1475, 2628, 79, 1392, 919, 2745, 181, 319, 1208, 2778, 359, 1573, 2581, 2393, 827, 1317]\n"
     ]
    }
   ],
   "source": [
    "mses = []\n",
    "best_doc_ids = []\n",
    "\n",
    "# Iterate through random queries\n",
    "print(\"Processing queries...\")\n",
    "for query_id in query_ids:\n",
    "    query = corpus[query_id]\n",
    "    query_embedding = encode_text(query)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    doc_scores = cosine_similarity([query_embedding], document_embeddings).flatten()\n",
    "    \n",
    "    # Get the second-highest score\n",
    "    doc_id = np.argsort(doc_scores)[-2]  # Second-highest score\n",
    "    best_doc_ids.append(doc_id)\n",
    "    \n",
    "    # Calculate MSE for evaluation\n",
    "    mse = np.mean((grouped_df.iloc[query_id][required_keys] - grouped_df.iloc[doc_id][required_keys])**2)\n",
    "    mses.append(mse)\n",
    "\n",
    "# Calculate the mean MSE\n",
    "mean_mse_t5b_baseline = np.mean(mses)\n",
    "print(f\"Mean MSE: {mean_mse_t5b_baseline}\")\n",
    "print(\"Best doc IDs:\", best_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Score: 0.5515936991289846\n",
    "\n",
    "- Best IDs: [2639, 1433, 1945, 504, 2100, 1801, 2631, 3638, 1732, 3606, 3084, 1491, 639, 1887, 200, 3080, 1028, 2957, 1711, 354, 3584, 2978, 3313, 2750, 1182, 644, 114, 2772, 2541, 470, 1417, 2348, 305, 1053, 430, 2307, 2655, 2530, 628, 2928, 0, 2141, 83, 249, 181, 436, 594, 557, 754, 126, 1303, 530, 1404, 2962, 1523, 1971, 372, 3191, 2969, 3503, 1438, 3521, 645, 594, 165, 1317, 3208, 622, 3502, 230, 812, 3100, 3135, 1352, 3282, 3387, 3362, 3595, 903, 3488, 2064, 639, 1935, 1319, 1475, 2628, 79, 1392, 919, 2745, 181, 319, 1208, 2778, 359, 1573, 2581, 2393, 827, 1317]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents into embeddings...\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "import sentencepiece\n",
    "import torch\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "model_name = \"t5-base\"  # Change to \"t5-base\" or \"t5-large\" for better results\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5Model.from_pretrained(model_name)\n",
    "\n",
    "# Ensure reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function to encode text into embeddings using T5\n",
    "def encode_text(text, max_length=512):\n",
    "    # Tokenize and prepare inputs\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    # Pass inputs through T5 encoder to get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encoder(**inputs)\n",
    "    # Mean pooling of the token embeddings\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Preprocess corpus to get embeddings\n",
    "print(\"Encoding documents into embeddings...\")\n",
    "document_embeddings = np.array([encode_text(doc) for doc in processed_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing queries...\n",
      "Mean MSE: 0.49376989779975416\n",
      "Best doc IDs: [2242, 1299, 3096, 1293, 216, 1918, 1108, 958, 808, 1941, 2119, 3407, 382, 3153, 3452, 2254, 3021, 1625, 551, 959, 2201, 1369, 943, 2352, 1087, 1353, 3657, 2481, 3557, 3453, 2572, 3333, 133, 1404, 390, 165, 1850, 1142, 2160, 309, 2105, 1290, 3565, 3122, 2730, 1262, 1515, 1005, 1379, 1465, 1218, 1435, 2665, 1318, 2483, 597, 1494, 582, 3538, 3573, 1029, 1432, 1846, 2782, 478, 1592, 1970, 867, 3358, 1321, 2887, 1877, 3295, 1321, 168, 283, 2321, 560, 1028, 117, 2259, 1262, 1505, 604, 2003, 3144, 1364, 2672, 549, 2393, 1133, 1979, 1408, 271, 3114, 2393, 2977, 778, 1454, 2552]\n"
     ]
    }
   ],
   "source": [
    "mses = []\n",
    "best_doc_ids = []\n",
    "\n",
    "# Iterate through random queries\n",
    "print(\"Processing queries...\")\n",
    "for query_id in query_ids:\n",
    "    query = processed_corpus[query_id]\n",
    "    query_embedding = encode_text(query)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    doc_scores = cosine_similarity([query_embedding], document_embeddings).flatten()\n",
    "    \n",
    "    # Get the second-highest score\n",
    "    doc_id = np.argsort(doc_scores)[-2]  # Second-highest score\n",
    "    best_doc_ids.append(doc_id)\n",
    "    \n",
    "    # Calculate MSE for evaluation\n",
    "    mse = np.mean((grouped_df.iloc[query_id][required_keys] - grouped_df.iloc[doc_id][required_keys])**2)\n",
    "    mses.append(mse)\n",
    "\n",
    "# Calculate the mean MSE\n",
    "mean_mse_t5b_preprocessed = np.mean(mses)\n",
    "print(f\"Mean MSE: {mean_mse_t5b_preprocessed}\")\n",
    "print(\"Best doc IDs:\", best_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Score: 0.49376989779975416\n",
    "\n",
    "- Best IDs: [2242, 1299, 3096, 1293, 216, 1918, 1108, 958, 808, 1941, 2119, 3407, 382, 3153, 3452, 2254, 3021, 1625, 551, 959, 2201, 1369, 943, 2352, 1087, 1353, 3657, 2481, 3557, 3453, 2572, 3333, 133, 1404, 390, 165, 1850, 1142, 2160, 309, 2105, 1290, 3565, 3122, 2730, 1262, 1515, 1005, 1379, 1465, 1218, 1435, 2665, 1318, 2483, 597, 1494, 582, 3538, 3573, 1029, 1432, 1846, 2782, 478, 1592, 1970, 867, 3358, 1321, 2887, 1877, 3295, 1321, 168, 283, 2321, 560, 1028, 117, 2259, 1262, 1505, 604, 2003, 3144, 1364, 2672, 549, 2393, 1133, 1979, 1408, 271, 3114, 2393, 2977, 778, 1454, 2552]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6713620455658073\n",
      "[1498, 2511, 115, 3603, 1125, 2355, 916, 655, 2124, 3435, 833, 3645, 2666, 209, 367, 2218, 2166, 2120, 1759, 558, 390, 2351, 1482, 3692, 2262, 2218, 3484, 1425, 3592, 2336, 1393, 1519, 3428, 3361, 992, 1095, 655, 1482, 3551, 3383, 1503, 3695, 3242, 2534, 1416, 564, 2485, 378, 1254, 1040, 2203, 2642, 3660, 2950, 1963, 1355, 2295, 987, 1193, 1075, 1663, 2666, 393, 2299, 1013, 402, 897, 3124, 733, 67, 476, 1932, 2457, 3115, 3155, 915, 2437, 979, 2120, 2850, 2115, 2846, 2265, 3240, 1788, 3660, 2775, 1, 2547, 671, 1047, 3219, 1487, 3013, 2727, 1203, 783, 3309, 2912, 1523]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Encode documents into embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight BERT-based model\n",
    "embeddings = model.encode(corpus)\n",
    "\n",
    "mses = []\n",
    "best_doc_ids = []\n",
    "\n",
    "# Iterate through random queries\n",
    "for query_id in query_ids:\n",
    "    query = corpus[query_id]\n",
    "    query_embedding = model.encode([query])\n",
    "    doc_scores = cosine_similarity(query_embedding, embeddings).flatten()\n",
    "    doc_id = np.argsort(doc_scores)[-2]  # Second-highest score\n",
    "    best_doc_ids.append(doc_id)\n",
    "    mse = np.mean((grouped_df.iloc[query_id][required_keys] - grouped_df.iloc[doc_id][required_keys])**2)\n",
    "    mses.append(mse)\n",
    "\n",
    "mean_mse_bert_baseline = np.mean(mses)\n",
    "print(mean_mse_bert_baseline)\n",
    "print(best_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Score: 0.6713620455658073\n",
    "\n",
    "- Best IDs: [1498, 2511, 115, 3603, 1125, 2355, 916, 655, 2124, 3435, 833, 3645, 2666, 209, 367, 2218, 2166, 2120, 1759, 558, 390, 2351, 1482, 3692, 2262, 2218, 3484, 1425, 3592, 2336, 1393, 1519, 3428, 3361, 992, 1095, 655, 1482, 3551, 3383, 1503, 3695, 3242, 2534, 1416, 564, 2485, 378, 1254, 1040, 2203, 2642, 3660, 2950, 1963, 1355, 2295, 987, 1193, 1075, 1663, 2666, 393, 2299, 1013, 402, 897, 3124, 733, 67, 476, 1932, 2457, 3115, 3155, 915, 2437, 979, 2120, 2850, 2115, 2846, 2265, 3240, 1788, 3660, 2775, 1, 2547, 671, 1047, 3219, 1487, 3013, 2727, 1203, 783, 3309, 2912, 1523]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6309224002773224\n",
      "[3625, 3712, 1641, 1547, 2495, 670, 2650, 976, 167, 5, 2436, 2607, 996, 2503, 2742, 1800, 1561, 1626, 121, 2999, 21, 1440, 927, 1257, 2357, 3584, 3657, 366, 1889, 1567, 3033, 1519, 3242, 774, 629, 2740, 1848, 2640, 458, 3699, 1288, 315, 893, 3122, 2644, 1053, 670, 3142, 3092, 3094, 10, 884, 1341, 1578, 2318, 3539, 2905, 437, 1446, 1259, 166, 3279, 566, 1214, 1257, 1852, 1918, 1389, 2880, 2686, 3556, 3147, 3625, 3429, 1117, 3420, 3477, 1073, 305, 815, 3571, 1257, 820, 1035, 1799, 3625, 45, 47, 38, 3606, 867, 2241, 996, 1551, 3013, 3279, 3029, 3387, 2092, 34]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Encode documents into embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight BERT-based model\n",
    "embeddings = model.encode(processed_corpus)\n",
    "\n",
    "mses = []\n",
    "best_doc_ids = []\n",
    "\n",
    "# Iterate through random queries\n",
    "for query_id in query_ids:\n",
    "    query = processed_corpus[query_id]\n",
    "    query_embedding = model.encode([query])\n",
    "    doc_scores = cosine_similarity(query_embedding, embeddings).flatten()\n",
    "    doc_id = np.argsort(doc_scores)[-2]  # Second-highest score\n",
    "    best_doc_ids.append(doc_id)\n",
    "    mse = np.mean((grouped_df.iloc[query_id][required_keys] - grouped_df.iloc[doc_id][required_keys])**2)\n",
    "    mses.append(mse)\n",
    "\n",
    "mean_mse_bert_preprocessed = np.mean(mses)\n",
    "print(mean_mse_bert_preprocessed)\n",
    "print(best_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- Score: 0.6309224002773224\n",
    "\n",
    "- Best IDs: [3625, 3712, 1641, 1547, 2495, 670, 2650, 976, 167, 5, 2436, 2607, 996, 2503, 2742, 1800, 1561, 1626, 121, 2999, 21, 1440, 927, 1257, 2357, 3584, 3657, 366, 1889, 1567, 3033, 1519, 3242, 774, 629, 2740, 1848, 2640, 458, 3699, 1288, 315, 893, 3122, 2644, 1053, 670, 3142, 3092, 3094, 10, 884, 1341, 1578, 2318, 3539, 2905, 437, 1446, 1259, 166, 3279, 566, 1214, 1257, 1852, 1918, 1389, 2880, 2686, 3556, 3147, 3625, 3429, 1117, 3420, 3477, 1073, 305, 815, 3571, 1257, 820, 1035, 1799, 3625, 45, 47, 38, 3606, 867, 2241, 996, 1551, 3013, 3279, 3029, 3387, 2092, 34]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
